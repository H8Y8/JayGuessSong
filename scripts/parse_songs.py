#!/usr/bin/env python3
"""
è§£æ songs.csv ä¸¦è½‰æ›æˆ Supabase seed.sql æ ¼å¼
æ ¹æ“š PRD songs è¡¨çµæ§‹ï¼š
- id: uuid (è‡ªå‹•ç”Ÿæˆ)
- title_zh: æ­Œæ›²ä¸­æ–‡åç¨±
- album: å°ˆè¼¯åç¨± (nullable)
- year: ç™¼è¡Œå¹´ä»½ (nullable)
- youtube_video_id: YouTube å½±ç‰‡ ID
- start_sec: èµ·å§‹ç§’æ•¸ (default: 0)
- duration_sec: æ’­æ”¾æ™‚é•· (nullable)
- difficulty: é›£åº¦ç­‰ç´š 1-3 (default: 1)
- is_active: æ˜¯å¦å•Ÿç”¨ (default: true)
"""

import csv
import re
import json
from urllib.parse import urlparse, parse_qs
from pathlib import Path


def extract_youtube_video_id(url: str) -> str | None:
    """å¾ YouTube URL æå– video ID"""
    try:
        parsed = urlparse(url)
        if parsed.hostname in ('www.youtube.com', 'youtube.com'):
            query = parse_qs(parsed.query)
            return query.get('v', [None])[0]
        elif parsed.hostname == 'youtu.be':
            return parsed.path[1:]
    except Exception:
        pass
    return None


def extract_title_zh(raw_title: str) -> str:
    """
    å¾åŸå§‹æ¨™é¡Œæå–ä¸­æ–‡æ­Œå
    æ ¼å¼: å‘¨æ°å€« Jay Chouã€æ­Œå English Nameã€‘Official MV
    """
    # åŒ¹é…ã€ã€‘ä¸­çš„å…§å®¹
    match = re.search(r'ã€(.+?)ã€‘', raw_title)
    if match:
        full_name = match.group(1)
        # å˜—è©¦åªå–ä¸­æ–‡éƒ¨åˆ†ï¼ˆç©ºæ ¼å‰ï¼‰
        parts = full_name.split(' ')
        if parts:
            # éæ¿¾æ‰ feat. ç­‰æ¨™è¨˜
            zh_name = parts[0]
            # ç§»é™¤å¯èƒ½çš„æ¨™é»
            zh_name = zh_name.strip()
            return zh_name
    return raw_title


def parse_songs_csv(csv_path: str) -> list[dict]:
    """è§£æ CSV ä¸¦è¿”å›æ­Œæ›²åˆ—è¡¨"""
    songs = []
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        # ç§»é™¤ BOM ä¸¦è™•ç†æ›è¡Œç¬¦
        content = f.read().replace('\r\n', '\n').replace('\r', '\n')
        
    # é‡æ–°è§£æ
    lines = content.strip().split('\n')
    reader = csv.DictReader(lines)
    
    for row in reader:
        raw_title = row.get('title', '').strip()
        url = row.get('url', '').strip()
        
        if not raw_title or not url:
            continue
            
        # æå– YouTube video ID
        video_id = extract_youtube_video_id(url)
        if not video_id:
            print(f"âš ï¸  ç„¡æ³•æå– video ID: {url}")
            continue
            
        # æå–ä¸­æ–‡æ­Œå
        title_zh = extract_title_zh(raw_title)
        
        # æ¸…ç†æ­Œåä¸­çš„ç‰¹æ®Šå­—ç¬¦
        title_zh = title_zh.replace('"', '').replace("'", "''")
        
        songs.append({
            'title_zh': title_zh,
            'youtube_video_id': video_id,
            'start_sec': 0,
            'difficulty': 1,
            'is_active': True
        })
    
    return songs


def generate_seed_sql(songs: list[dict], output_path: str):
    """ç”Ÿæˆ Supabase seed.sql"""
    
    sql_lines = [
        "-- Jay Guess é¡Œåº«ç¨®å­è³‡æ–™",
        f"-- ç”Ÿæˆæ™‚é–“: 2025-12-30",
        f"-- ç¸½æ­Œæ›²æ•¸: {len(songs)}",
        "",
        "-- æ¸…ç©ºç¾æœ‰è³‡æ–™ (å¯é¸)",
        "-- TRUNCATE TABLE songs RESTART IDENTITY CASCADE;",
        "",
        "INSERT INTO songs (title_zh, youtube_video_id, start_sec, difficulty, is_active)",
        "VALUES"
    ]
    
    values = []
    for song in songs:
        value = f"  ('{song['title_zh']}', '{song['youtube_video_id']}', {song['start_sec']}, {song['difficulty']}, {str(song['is_active']).lower()})"
        values.append(value)
    
    sql_lines.append(',\n'.join(values) + ';')
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(sql_lines))
    
    print(f"âœ… å·²ç”Ÿæˆ SQL: {output_path}")


def generate_json(songs: list[dict], output_path: str):
    """ç”Ÿæˆ JSON æ ¼å¼ï¼ˆæ–¹ä¾¿é™¤éŒ¯å’Œé è¦½ï¼‰"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(songs, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ç”Ÿæˆ JSON: {output_path}")


def main():
    base_dir = Path(__file__).parent.parent  # ä¸Šå±¤ç›®éŒ„ (JayGuessSong/)
    csv_path = base_dir / 'songs.csv'
    
    if not csv_path.exists():
        print(f"âŒ æ‰¾ä¸åˆ° {csv_path}")
        return
    
    print(f"ğŸ“‚ è®€å– {csv_path}")
    songs = parse_songs_csv(str(csv_path))
    
    print(f"ğŸµ è§£æåˆ° {len(songs)} é¦–æ­Œæ›²")
    
    # æª¢æŸ¥é‡è¤‡çš„ video ID
    video_ids = [s['youtube_video_id'] for s in songs]
    duplicates = set([vid for vid in video_ids if video_ids.count(vid) > 1])
    if duplicates:
        print(f"âš ï¸  ç™¼ç¾é‡è¤‡çš„ video ID: {duplicates}")
        # ç§»é™¤é‡è¤‡
        seen = set()
        unique_songs = []
        for song in songs:
            if song['youtube_video_id'] not in seen:
                seen.add(song['youtube_video_id'])
                unique_songs.append(song)
        songs = unique_songs
        print(f"ğŸ”„ ç§»é™¤é‡è¤‡å¾Œå‰©é¤˜ {len(songs)} é¦–æ­Œæ›²")
    
    # ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆ
    supabase_dir = base_dir / 'supabase'
    supabase_dir.mkdir(exist_ok=True)
    
    generate_seed_sql(songs, str(supabase_dir / 'seed.sql'))
    generate_json(songs, str(base_dir / 'songs_parsed.json'))
    
    # é¡¯ç¤ºå‰ 5 é¦–æ­Œæ›²é è¦½
    print("\nğŸ“‹ å‰ 5 é¦–æ­Œæ›²é è¦½:")
    for i, song in enumerate(songs[:5], 1):
        print(f"  {i}. {song['title_zh']} (ID: {song['youtube_video_id']})")
    
    print(f"\nâœ… å®Œæˆï¼å…± {len(songs)} é¦–æ­Œæ›²")


if __name__ == '__main__':
    main()
